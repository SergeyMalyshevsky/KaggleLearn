{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Autotunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CSVData:\n",
    "    \n",
    "    def __init__(self, train_csv, test_csv, index_col, label_col):\n",
    "        # Read the data\n",
    "        X = pd.read_csv(train_csv, index_col=index_col)\n",
    "        X_test_full = pd.read_csv(test_csv, index_col=index_col) \n",
    "        \n",
    "        # Remove rows with missing target, separate target from predictors\n",
    "        X.dropna(axis=0, subset=[label_col], inplace=True)\n",
    "        y = X.SalePrice              \n",
    "        X.drop([label_col], axis=1, inplace=True)\n",
    "\n",
    "        # Break off validation set from training data\n",
    "        X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "        # \"Cardinality\" means the number of unique values in a column\n",
    "        # Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "        low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                                X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "        # Select numeric columns\n",
    "        numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "        # Keep selected columns only\n",
    "        my_cols = low_cardinality_cols + numeric_cols\n",
    "        X_train = X_train_full[my_cols].copy()\n",
    "        X_valid = X_valid_full[my_cols].copy()\n",
    "        X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "        # One-hot encode the data (to shorten the code, we use pandas)\n",
    "        X_train = pd.get_dummies(X_train)\n",
    "        X_valid = pd.get_dummies(X_valid)\n",
    "        X_test = pd.get_dummies(X_test)\n",
    "        X_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n",
    "        X_train, X_test = X_train.align(X_test, join='left', axis=1)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid\n",
    "        self.X_test = X_test\n",
    "        \n",
    "    def train_test_split(self):\n",
    "        return self.X_train, self.X_valid, self.y_train, self.y_valid\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[XGBoost Tunning](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class XGBAutoTunning:\n",
    "    \n",
    "    def __init__(self, csv_data):\n",
    "        self.start = time.time()\n",
    "        self.random = False\n",
    "        \n",
    "        self.X_train, self.X_valid, self.y_train, self.y_valid = csv_data.train_test_split()\n",
    "        \n",
    "        self.learning_rate = 0.1\n",
    "        self.n_estimators = 1000\n",
    "        self.max_depth = 5\n",
    "        self.min_child_weight = 1\n",
    "        self.gamma = 0\n",
    "        self.subsample = 0.8\n",
    "        self.colsample_bytree = 0.8\n",
    "        self.objective ='reg:linear'\n",
    "        self.reg_alpha = 0\n",
    "        \n",
    "    def create_model(self):\n",
    "        self.model = XGBRegressor(\n",
    "                         learning_rate=self.learning_rate,\n",
    "                         n_estimators=self.n_estimators,\n",
    "                         max_depth=self.max_depth,\n",
    "                         min_child_weight=self.min_child_weight,\n",
    "                         gamma=self.gamma,\n",
    "                         subsample=self.subsample,\n",
    "                         colsample_bytree=self.colsample_bytree,\n",
    "                         objective=self.objective,\n",
    "                         nthread=4,\n",
    "                         scale_pos_weight=1,\n",
    "                         seed=27,\n",
    "                         reg_alpha=self.reg_alpha,\n",
    "                         silent=True)        \n",
    "        return self.model\n",
    "    \n",
    "    def model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def best_params(self, test_params):\n",
    "        if self.random:\n",
    "            result = {}\n",
    "            for key, value in test_params.items():\n",
    "                result[key] = random.choice(value)\n",
    "            return result\n",
    "        else:\n",
    "            model = GridSearchCV(estimator = self.model, param_grid = test_params)\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            return model.best_params_\n",
    "        \n",
    "    \n",
    "    def best_model(self):\n",
    "        # find max_depth and min_child_weight \n",
    "        param_test1 = {\n",
    "             'max_depth': range(3,10,2),\n",
    "             'min_child_weight': range(1,6,2)\n",
    "        }\n",
    "        self.create_model()\n",
    "        best_param = self.best_params(test_params=param_test1)\n",
    "        self.max_depth = best_param['max_depth']\n",
    "        self.min_child_weight = best_param['min_child_weight']\n",
    "        \n",
    "        param_test2 = {\n",
    "             'max_depth':[best_param['max_depth']-1, best_param['max_depth'], best_param['max_depth']+1],\n",
    "             'min_child_weight':[best_param['min_child_weight']-1, best_param['min_child_weight'], best_param['min_child_weight']+1]\n",
    "        }\n",
    "        self.create_model()\n",
    "        best_param2 = self.best_params(test_params=param_test2)\n",
    "        self.max_depth = best_param2['max_depth']\n",
    "        self.min_child_weight = best_param2['min_child_weight']\n",
    "        \n",
    "        param_test3 = {}\n",
    "        if best_param2['min_child_weight'] == best_param['min_child_weight'] + 1:\n",
    "            param_test3['min_child_weight'] = range(best_param2['min_child_weight'], best_param2['min_child_weight'] + 8, 2)\n",
    "            \n",
    "        if best_param2['max_depth'] == best_param['max_depth'] + 1:\n",
    "            param_test3['max_depth'] = range(best_param2['max_depth'], best_param2['max_depth'] + 8, 2)\n",
    "        \n",
    "        self.create_model()\n",
    "        if param_test3:\n",
    "            best_param = self.best_params(test_params=param_test3)\n",
    "            if 'min_child_weight' in best_param:\n",
    "                self.min_child_weight = best_param['min_child_weight']\n",
    "            if 'max_depth' in best_param:\n",
    "                self.min_child_weight = best_param['max_depth']              \n",
    "        \n",
    "        # find gamma\n",
    "        param_test4 = {\n",
    "            'gamma':[i/10.0 for i in range(0,5)]\n",
    "        }\n",
    "        self.create_model()\n",
    "        best_param = self.best_params(test_params=param_test4)\n",
    "        self.gamma = best_param['gamma']\n",
    "        \n",
    "        # find subsample and colsample_bytree\n",
    "        param_test5 = {\n",
    "            'subsample':[i/10.0 for i in range(6,10)],\n",
    "            'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "        }\n",
    "        self.create_model()\n",
    "        best_param = self.best_params(test_params=param_test5)\n",
    "        self.subsample = best_param['subsample']\n",
    "        self.colsample_bytree = best_param['colsample_bytree']\n",
    "        \n",
    "        subsample_range = range( int(abs(self.subsample * 100 - 5)), int(self.subsample * 100 + 10), 5 )\n",
    "        colsample_bytree_range = range( int(abs(self.subsample * 100 - 5)), int(self.subsample * 100 + 10), 5 )\n",
    "        param_test6 = {\n",
    "            'subsample':[i/100.0 for i in subsample_range],\n",
    "            'colsample_bytree':[i/100.0 for i in colsample_bytree_range]\n",
    "        }\n",
    "        self.create_model()\n",
    "        best_param = self.best_params(test_params=param_test6)\n",
    "        self.subsample = best_param['subsample']\n",
    "        self.colsample_bytree = best_param['colsample_bytree']\n",
    "        \n",
    "        # find reg_alpha\n",
    "        param_test7 = {\n",
    "            'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "        }\n",
    "        self.create_model()\n",
    "        best_param = self.best_params(test_params=param_test7)\n",
    "        self.reg_alpha = best_param['reg_alpha']\n",
    "        \n",
    "        param_test8 = {\n",
    "            'reg_alpha':[0, self.reg_alpha*0.1, self.reg_alpha*0.5, self.reg_alpha, self.reg_alpha*5]          \n",
    "        }\n",
    "        self.create_model()\n",
    "        best_param = self.best_params(test_params=param_test8)\n",
    "        self.reg_alpha = best_param['reg_alpha']\n",
    "        \n",
    "        # Set learning rate and increase n_estimators\n",
    "        self.learning_rate = 0.01\n",
    "        self.n_estimators = 5000\n",
    "        self.create_model()\n",
    "    \n",
    "    def fit(self):\n",
    "        self.model.fit(self.X_train, self.y_train, \n",
    "             early_stopping_rounds=500, \n",
    "             eval_set=[(self.X_valid, self.y_valid)], \n",
    "             verbose=False)\n",
    "    \n",
    "    def accuracy(self):\n",
    "        print(self.model)\n",
    "        kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        scores = -1 * cross_val_score(self.model, self.X_train, self.y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "        print(scores)\n",
    "        print(\"Average MAE score: \", scores.mean())\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.model.predict(self.X_valid)\n",
    "        # Calculate MAE\n",
    "        mae = mean_absolute_error(self.y_valid, predictions)\n",
    "        # Print MAE\n",
    "        print(\"Mean Absolute Error: \", mae)\n",
    "    \n",
    "    def tunning(self, random=False):\n",
    "        self.random = random\n",
    "        self.best_model()\n",
    "        self.fit()\n",
    "        self.end = time.time()\n",
    "        print('Execution time is ' + str(self.end - self.start) + ' seconds')\n",
    "        print(self.model)\n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use XGBAutoTunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\kagglelearn\\venv\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time is 19.136436462402344 seconds\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.65, gamma=0.4,\n",
      "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
      "             max_depth=4, min_child_weight=8, missing=None, n_estimators=5000,\n",
      "             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,\n",
      "             reg_alpha=0.005, reg_lambda=1, scale_pos_weight=1, seed=27,\n",
      "             silent=True, subsample=0.7, verbosity=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.65, gamma=0.4,\n",
       "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
       "             max_depth=4, min_child_weight=8, missing=None, n_estimators=5000,\n",
       "             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0.005, reg_lambda=1, scale_pos_weight=1, seed=27,\n",
       "             silent=True, subsample=0.7, verbosity=1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_col='Id'\n",
    "label_col='SalePrice'\n",
    "csv_data = CSVData(train_csv='../input/train.csv', test_csv='../input/test.csv', index_col=index_col, label_col=label_col)\n",
    "X_test = csv_data.X_test\n",
    "\n",
    "xgba = XGBAutoTunning(csv_data)\n",
    "xgba.tunning(random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.65, gamma=0.4,\n",
      "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
      "             max_depth=4, min_child_weight=8, missing=None, n_estimators=5000,\n",
      "             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,\n",
      "             reg_alpha=0.005, reg_lambda=1, scale_pos_weight=1, seed=27,\n",
      "             silent=True, subsample=0.7, verbosity=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\kagglelearn\\venv\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "d:\\kagglelearn\\venv\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "d:\\kagglelearn\\venv\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "d:\\kagglelearn\\venv\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17333.02572449 14043.00719485 16664.10022703 14440.05597841\n",
      " 15788.87692798]\n",
      "Average MAE score:  15653.813210550326\n",
      "Mean Absolute Error:  15586.836271939212\n"
     ]
    }
   ],
   "source": [
    "xgba.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = xgba.predict(X_test)\n",
    "\n",
    "# Save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({index_col: X_test.index, label_col: preds_test})\n",
    "output.to_csv('../output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
